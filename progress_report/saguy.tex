As detailed in the inception report, the original plan was to have two complete iterations by the 05/11 and 12/11 respectively. In practice, we completed one iteration which combined the tasks for both iterations by 12/11. Referring back to the inception report, our program was to be implemented only for the restricted language of $\mathcal{AL}$ (A particular description logic). Only in the second iteration, where we to extend the systems functionality to deal with the more expressive $\mathcal{ALC}$ logic. After discussing the matter with our supervisor we found the difference in implementation is relatively minor. In particular, the main difference is the fact that $\mathcal{ALC}$ allows for any concept negation whereas $\mathcal{AL}$ only allows atomic concept negation. This, both in the implementation and testing sides, was a relatively small addition and required little effort. We therefore decided on producing one iteration which would ultimately achieve a combined result of the first and second iterations, i.e a program that deals with all $\mathcal{ALC}$ (rather then $\mathcal{AL}$) concepts.

In order to provide a representation for concepts, models and proofs for $\mathcal{ALC}$ logic we produced three files in haskell: model.hs, proof.hs and signature.hs. The first provides the data type for model; this includes a domain which is a set of individuals in the model (represented as integers), a set of unary relations (i.e. the assignment of concepts to a subset of the domain) and a set of binary relations (which is a string naming the relation and an assignment to the subset of Domain x Domain). The proof was represented inductively using a proofTree definition. The signature provided a representation for concepts corresponding directly to the concepts expressible in $\mathcal{ALC}$. This was made as extensible as possible for other description logics that we could think of at this stage. 

We separated the work roughly equally between the group members, where Michal worked on the Model checker, Ka Wai on the proof checker, and Saguy and Jannis on the Proof/Model search. For each of these parts two files were created: an implementation file and a test suit which extensively tested the implementation primarily through the use of unit testing (Hunit) (for example $ProofSearch.hs$ and $ProofSearch\_test.hs$). For the Model checker we provided a function, that given a model, knowledge base and set of concepts, checks whether the model provided is correct with respect to the knowledge base and the set of concepts which need be satisfiable. We also provided an error-reporting mechanism that tells what part of the model is not satisfiable if the model happens to be an incorrect one. 

Similarly, we provide a proof checker, that takes as an input a proof, a list of concepts, and a knowledge base; and checks whether the proof is correct (i.e the list of concepts is unsatisfiable with respect to the knowledge base). Finally, we produced an algorithm which takes as an input a set of concepts and a knowledge base, and provides either a model if the concepts are satisfiable and a proof in they are unsatisfiable. The algorithm itself is using sufficient information to generate either a proof or a model. As mentioned earlier, we provided test suits for each file, and, as correctness is critical for this project, we will constantly update these files and add additional tests for the functionality of these programs in more complicated cases.  

At times we found that it was difficult to meet all of the requirements in the exact deadline set internally and we had to adjust this deadline. However, we managed to quickly resolve any issues, mainly due to the frequent meetings we have (as part of our development method) that were set every Monday from 12:00 to 14:00 and on Friday from 16:00 to 17:00 as well as a weekly meeting with our supervisor. The main aim was to get reasonably sufficient amount of testing completed by Monday 08/11 and to coordinate the activities together. The meeting on Monday 08/11 helped us clarify the remaining tasks and redistribute them (who much and who does what tests) and decide on when we could integrate the program together (agreed to be Thursday 11/11). In addition, we found, as expected, that in this iteration a lot of work was carried to have a fully functional program solving the high risks elements of our project. This, at times, meant that other work was pushed aside; and frequent internal deadlines helped us achieve high productivity, with each member expected to finish his/her part on time. A potential problem that we may incur is that the current test suits may not provide sufficient testing for the program to make sure it is completely correct. For this reason, we will constantly update the test suits in subsequent iterations.

% SHOULD WE PERHAPS INCLUDE FORMAL REASONING
