\subsection{Technology Used}

We used several tools and languages in our program, with the main programming language being Haskell. Other than Haskell, the parser generator, Happy, was used to produce the Haskell code for the parser component of the program. For the output of models and proofs into more readable format, Graphviz and Latex were used respectively.

Haskell, was chosen as the main programming language primarily because Haskell's pattern matching naturally implements the handling of cases (such as proof rules and formulas) together with way we decided to structure the program into the main components. This way work on the different components can be implemented independently and simultaneously. Other reasons why Haskell was more appealing than other programming languages considered (Java, C, C++) were Haskell's compact code and each team member's relative levels of programming skills and experience in each language.

With no prior experience in programming a parser, we considered using Happy and Parsec in implementing the parser component for user input and benchmark files. Happy is an LR (bottom-up) parser generator and requires Happy to be run to generate the Haskell code for the parser, whilst Parsec is a LL (top-down) parser library for Haskell so does not require this pre-processing stage.

All members of our team have not used either of these before and so the decision to use Happy was based upon the available documentation. Happy's documentation was clearer and we observed that it involved simpler hand-written code. Hence, we believed the parser would be completed in a shorter amount of time by using Happy. The only disadvantage is that Happy needed to be run after any changes to the Happy file to generate, but this would only be required in the development stages and not when used by end users.

For the output of proofs, 

On the other hand, for the output of models, Graphviz was used. Graphviz is an open source graph visualisation software that have tools and libraries

\subsection{Technical Challenges}

We encountered a few technical challenges throughout the development of this project. The most problematic were the implementation of caching in the proof/model search component and in extending our program to allow users to express specific individuals that must exist and to reason with. These problems were addressed by our frequent informal meetings, in which we discussed the difficulties arisen and finding ways to resolve them.

In the early iterations, we discovered the potential problem of cycles in the input global assumptions, which causes complications in the implementation of proof search, model construction and model checking. To prevent the proof/model search component from potentially becoming non-terminating, we decided to implement a mechanism for caching the sets of concepts already expanded and their resulting proof or model.

Extending the already written proof/model search component to cover this was very challenging because proof construction and model construction are performed simultaneously in our implementation. By implementing caching, the code became much more complex than before and as a result, it was difficult to reason the correctness of code and to debug for very specific cases.

This issue was addressed in our frequent group meetings by discussion, using code reviews, and bug tracking system. In this way, other members of the group not assigned to the implementation of the proof/model search aided in locating and fixing bugs.

Another challenge we faced was when attempting to extend the program to express a new sort, individuals, in logic, enabling a specific individual to be expressed and reasoned with. In our meetings, we discussed the theoretical issues and how this would be implemented. The proof theory for individuals did not work with our design, and all data structures and components of the program needed to go through a great deal of changes. Given the limited time, it was decided that we will not implement this extension, but rather implement the other key requirements set for this project.

\subsection{Software Development and Testing Methods}

To minimise risk and allow the project to adapt to changes quickly when difficulties occur, a mixture of software development methods was adopted together with extensive testing methods. In addition, a variety of tools was used and code management policies were established to ensure quality and correctness of code.


The software development method adopted was a blend of mainly eXtreme Programming and Scrum, which were suitable for this project. Agile software development methods meant by using short (1 week) iterations and ensured there was a working product at the end of each iteration.

The test-driven nature of eXtreme Programming was a core part of our project development since the integrity of code is very important and directly affects whether the key requirements are met. Therefore, test suites for unit testing and test cases well known in the proof-search-writer community were used to test for correctness and performance. These unit tests were written before the coding of functions or simultaneously, and will be tested as they are implemented.

To complement with the unit tests, we also made use of hlint, grey-box testing after each significant change and black-box testing to test our program with large complex inputs. We used hlint, a program similar to lint, that analyses Haskell programs and suggests changes, to ensure a good and consistent style throughout the program. This not only ensured that the source code is easier to read and understand, but also made programming mistakes less likely.

We integrated these tools into a validation process. We ran hlint and unit tests before submitting the code to the repository. Then we gave code reviews and used grey-box testing to improve the quality of the code and find bugs that weren't found before. On a regular basis we ran the black-box tests on the whole program, discovering issues that were missed in the previous steps. This whole setup gave us high confidence that the program is working correctly.

Due to the possible difficulty of forming the algorithms in our project, we gave priority to having a simple solution with clear code first, a feature of eXtreme Programming, to guarantee the existence of a working version of the program. This way there was a working version even when using a different implementation.

Extensive code reviews were essential for improving quality of code and for all team members to understand each others code, under eXtreme Programming. It also led to the discovery of some small programming mistakes in the reviews. Hence the project was hosted on Google Code throughout development, chosen for its features allowing easy submission for code reviews by other team members.

In addition, Google Code integrated well with our choice of version control, Mercurial and provided an issue tracker, which we used to report bugs to address them. The benefits of using Mercurial were it allows local commits before pushing the changesets to the repository, with relatively easy-to-use graphical user interface extensions; and it is more flexible than other version controls, for example Git.

From eXtreme Programming, pair programming was used for overlapping areas and more complex parts of the program; and also where sections of the code, implemented by different team members, meet. This ensured code written by different members would be as compatible as possible. Otherwise, each member was mainly responsible for a certain section of the program.

As in Scrum, we kept a product backlog, sprint backlog and also a `burn down list' to measure the actual progress and velocity of the overall project and iterations against. This gave each member clear specific tasks to complete that contributed towards meeting the requirements set for the project.

Regular scrum-like meetings were undertaken when we have a group meeting at least twice a week, focusing on:

\begin{itemize}
\item What has each member done since the last meeting (progress with respect to the assigned tasks)?
\item What will each member do until the next meeting?
\item Any technical and theoretical issues that have arisen and how to resolve them.
\end{itemize}

These short meetings encouraged progression of the project and highlighted technical problems early on. Most knowledge transfer was done through these meetings and also regular meetings with our supervisor, where the more difficult concepts and later iterations were discussed.

For the first few iterations, there were many extra meetings as well as the scrum-like meetings, in which we discussed the possible algorithms and proof rules used and agreed on specific design requirements of each component. This was to make each member's code as compatible as possible; and it is important for everyone to understand the code under eXtreme Programming.

From considering Unified Process method, we decided to implement the high risk elements in early iterations so that the most critical key requirements and most difficult aims are achieved first before implementing the features with a lower risk.

\subsection*{Testing Methods}

Since a test-driven development method was adopted, unit testing was done at all stages of development, using HUnit (a unit testing framework for Haskell). To complement unit testing, other testing methods included the use of grey-box and black-box testing.

Unit tests were written before the implementation of functions or simultaneously; and test suites are run regularly, especially after any changes to code, to ensure the program still works as expected. All areas of code were tested in this way individually: model/proof construction; model checking; proof checking; parsing of user input and test files; and the outputting of the constructed proof and model.

* Estimate number of unit tests here *

Although writing extensive sets of unit tests that cover all possible cases took almost as much time as implementing the code itself, there were many benefits from this approach. By simultaneously writing the unit tests, it helped programmers think about and have a clear view of the expected/desired output of functions. This made coding of functions easier and members were able to identify mistakes quickly. Most of the time, bugs were found as the functions were implemented and were therefore corrected immediately.

When moving on to code other parts of the program, we have confidence that tested code is correct and produces expected results when given expected inputs. As a result, we were able to highlight differences in expectations between members writing different parts of the program for discussion in our meetings and come to an agreed solution.

In particular, having extensively tested proof checker and model checker by unit tests enabled these components of the program to be used with high confidence in other types of testing. The proof and model checkers are important in these other tests to check the results produced, for the automatically generated complex input cases, by the proof and model construction part of the program, which is very complicated and much harder to test simply by unit tests due to its exploratory nature.

Overall, unit testing revealed most bugs early on and immediately as code was being implemented or underwent a change, before code became more complex and difficult to resolve. Unit tests were written for code coverage, ensuring all input cases were considered and our code produces the expected results.

These bugs included cases that were not covered properly, errors in the code, and bugs introduced due to changes in code.

Unit testing immediately revealed some forgotten cases in the initial code, such as:

\begin{itemize}
\item When falsity was in the set of inputs for the proof and model construction, a model was produced rather than the expected proof containing only falsity to show unsatisfiability.
\item In the case where top (truth) was among many concepts in the list of inputs, an incorrect model with was constructed and returned without ensuring all other concepts in the list of inputs were also satisfiable.
\item The possibility of cycles in the input global assumptions was not dealt with by the proof and model construction stage.
\end{itemize}

Some mistakes in code that may have been overlooked without our approach to unit testing:

\begin{itemize}
\item Proofs should deal with set of concepts not containing any duplicates, however, the proof searcher produced a result containing duplicated concepts as a result of applying the proof rule for Exists since a specific case was not considered. This was where the inputs contained:
\begin{itemize}
\item An Exists concept stating there exists a successor to a particular relation that satisfies a concept
\item A Forall concept stating all successors of the same relation must satisfy the same concept specified by the exists concept
\end{itemize}
\end{itemize}

After some changes to code, some bugs were introduced since the effect of the changes on all cases was not considered. For example:

\begin{itemize}
\item After combining the separate parsers for user input and benchmark files to reuse functions, the parsing for one of the benchmark files was incorrect and needed to have further changes.
\end{itemize}

Although the unit tests were very useful and helped us find many bugs an early stage, they don't help with more complex bugs. This is why we made use of grey/black box testing.

After each significant change we used grey-box testing to validate these changes, i.e. we tried, using our knowledge of the inner workings of the program, to find possible problems. This sometimes led to the discovery of small issues, for example a forgotten case.

Furthermore, we used two different ways of black-box testing. We wrote a small program that automatically generates large test cases, runs the proof/model searcher on them, and then either the proof or the model checker on the result. We also wrote a parser, so we could read complex `benchmark concepts' that are available from the proof searcher community, and test these in the same way.

Both these approaches turned out to be very useful and helped us discover some rather obscure bugs (e.g. bugs that would only occur if concepts were nested in a very particular and complex way). These kinds of tests can also be understood as stress testing of our program, because the cases are larger than we expect normal user input to be and are very likely to test concepts that go beyond what users would normally input.

\subsection{Comparisons of Plans and Achievements}

\subsection*{Iteration 1 (start 22/10/10, end 12/11/10)}

The original plan was to have two complete iterations by the 05/11/10 and 12/11/10 respectively. In practice, we completed one iteration which combined the tasks for both iterations by 12/11/10.

We intended to produce a working program that, given an AL concept, produces either a model for that concept or a proof showing its unsatisfiability, and both proof and model can be checked for correctness by the program. Only in the second iteration, where we to extend the systems functionality to deal with the more expressive ALC logic. However, we found that a vast amount of refactoring and redesign will be required, if we were to move from AL to ALC in the second iteration.

We agreed that, both in the implementation and testing sides, it would be wise to produce one iteration, which would ultimately achieve a combined result of the first and second iterations, i.e. a program that deals with all ALC (rather then AL) concepts.

In the beginning of the iteration all team members spent time acquiring a theoretical understanding of the task by reading journal articles on description logic and proof/model searching. Everyone needed to have a good understanding of the underlying theory and identify suitable algorithms and adapt them to our setup.

* Reference articles here *

Overall, we were able to complete the iteration successfully and we discovered the problem of cycles in the global assumptions, causing non-termination in the proof/model search component. The tasks for our first iteration and our achievements were as follows:

\begin{center}
\begin{longtable}{| p{7cm} | p{8cm} |}
\hline
\textbf{Tasks} & \textbf{Achievements} \\
\hline
Specifications of concept, model and proofs as Haskell data types.
& Specifications of concepts, models and proofs from ALC logic as Haskell data types.\\
\hline
Implementation of proof/model search for ALC.
& Implementation of a model/proof searching algorithm for ALC. Tested and found to be correct for all cases for which it terminates. \\
\hline
Implementation of model checker for ALC.
& Implementation of a model checking algorithm for ALC that checks if a model is correct and additionally provides an error report if the model is incorrect.\\
\hline
Implementation of proof checker for ALC.
& Implementation of a proof checking algorithm for ALC that checks if a proof is correct and additionally provides an indication of the position where the proof is incorrect if it is incorrect.\\
\hline
& Complete test suites for the proof/model search, proof checker and model checker, as a result of test-driven development method.\\
\hline
\end{longtable}
\end{center}

\subsection*{Iteration 2 (start 12/11, end 19/11)}

In the draft schedule, we had planned to extend the system's functionality to cover a more expressive logic in this iteration. However, from implementing the first iteration, we discovered there was the potential problem of cycles in the input global assumptions, which causes complications in the implementation of proof search, model construction and model checking.

As a solution we decided to use a mechanism for caching the set of concepts visiting and their resulting model of proof. We decided to deal with in this iteration as addressing this potential problem later, after other extensions have been added, had the risk of causing the solution to be more difficult to attain and test.

At this stage, we decided to run all tests together by using cabal, so it would be convenient for all team members to see test results for all components of the program.

In this iteration, there was difficulty in scheduling meetings and much time was needed to read and understand how to implement caching, especially for proof and model construction simultaneously. Consequently, the completion of this iteration was hindered.

\begin{center}
\begin{longtable}{| p{7cm} | p{8cm} |}
\hline
\textbf{Tasks} & \textbf{Achievements} \\
\hline
Implementation of caching so that the proof/model search terminates and returns a correct result when given global assumptions containing cycles. & Reading and understanding the theory and how it would be implemented, but unimplemented.\\
\hline
Set up cabal for package management. & Completed.\\
\hline
Set up cabal for running all the tests together. & Completed.\\
\hline
\end{longtable}
\end{center}

\subsection*{Iteration 3 (start 19/11, end 26/11)}

In this iteration, we had planned to extend the program to be able to express a new sort, individuals, in logic. This was to enable users to express and reason with a specific individual in the domain. However, since the previous iteration did not run smoothly, the task of implementation of caching was extended to this iteration.

To enable black-box testing to be performed with large complex input cases, a program that automatically generates large test cases; and a parser to extract large complex concepts from benchmark files were implemented. Hence the implementation of the parser for user input was moved to this iteration to be developed at the same time, rather than implemented as planned in the last iteration.

\begin{center}
\begin{longtable}{| p{7cm} | p{8cm} |}
\hline
\textbf{Tasks} & \textbf{Achievements} \\
\hline
Implementation of caching so that the proof/model search terminates and returns a correct result when given global assumptions containing cycles. & Implemented with bugs in cases with cycles in the input global assumptions.\\
\hline
Implement parser for benchmark files and user input	. & Implemented a parser with different lexical analysers for benchmark files and user input. Unit tests for the lexical analyser component written.\\
\hline
Clearer and consistent test suite setup and messages returned for all components. & Completed by refactoring common code/test cases in different components of the program and creating functions to aid the coding of unit tests.\\
\hline
Implementation of test case generator. & Implemented and revealed some obscure bugs.\\
\hline
\end{longtable}
\end{center}

\subsection*{Iteration 4 (start 26/11, end 3/12)}

This iteration was intentionally planned to extend the program to cover counting in logic only if the previous iteration progressed well. As expected, there were problems due to the technical nature of this project and difficulty of formulating the algorithm for caching with the existing code in the proof/model search component. Therefore, we started to plan how to extend our program to express individuals in the domain in this iteration rather than in the previous.

After much time spent on reading and discussing the theory on individuals, we found that our code needed to undergo a substantial amount of change to successfully. Given the limited time, we decided not to continue with this extension.

Most of the time was invested in fixing bugs in proof/model search and testing in this iteration. The tasks and achievements were:

\begin{center}
\begin{longtable}{| p{7cm} | p{8cm} |}
\hline
\textbf{Tasks} & \textbf{Achievements} \\
\hline
Fix the bug highlighted in the previous iteration, in the proof/model search using caching.	& Fixed this bug, but black box-testing revealed obscure bugs existed in this component.\\
\hline
Extend program components to express individuals.	& Forgone.\\
\hline
Implement tests for actual parsing of files and user input. & Completed.\\
\hline
\end{longtable}
\end{center}

\subsection*{Iteration 5 (start 3/12, end 10/12)}

For the last iteration, we integrated the additional features as planned, including the output of proofs and models into human readable format, and implementing the search for the shortest proof/smallest model by the proof/model search component. In addition, we were able to complete and fix the remaining bugs in the proof/search component. The final tasks and achievements:

To complete the project, we prepared the program for open-source deployment was done in this iteration by writing the main function and user interface.

\begin{center}
\begin{longtable}{| p{7cm} | p{8cm} |}
\hline
\textbf{Tasks} & \textbf{Achievements} \\
\hline
Fix bugs in proof/model search. & Completed.\\
\hline
Implement output model component to output a given model as a graph. & Completed.\\
\hline
Implement output proof component to output proof as a Latex file. & Completed.\\
\hline
Find shortest proof/smallest model. & Completed. * Did we do this? *\\
\hline
Implement the main function. & Completed.\\
\hline
\end{longtable}
\end{center}

\subsection{Code Lengths}

During the development of the program, we found that most of the test suites were of a larger length of lines than their associated components, due to our extensive testing methods. Upon completion of all the iterations, our program consisted of the following components and lengths of code, which we believe to be relatively short as a result of using Haskell as our programming language:

\begin{center}
  \begin{tabular}{|l|l|c|}
\hline
\textbf{Component} & \textbf{Modules/Files} & \textbf{Code Length (lines)} \\
\hline
    Common code & Signature.hs & 118 \\
    (utilities and data structures) & Signature\_test.hs & 128 \\
     & Model.hs & 88 \\
     & Model\_test.hs & 36 \\
     & Proof.hs & 24 \\
     & ProofUtils.hs & 107 \\
     & Proof\_test.hs & 85 \\
\hline
    Parser & Parser.y & 199 \\
     & Parser.hs (generated code) & 912 \\
     & Parser\_test.hs & 472 \\
\hline
    Proof/Model Search & ProofSearch.hs & 161 \\
     & ProofSearch\_test.hs & 342 \\
\hline
   Model Checker & ModelChecker.hs & 136 \\
    & ModelChecker\_test.hs & 406 \\
\hline
   Proof Checker & ProofChecker.hs & 144 \\
    & ProofChecker\_test.hs & 297 \\
\hline
   Output Model & OutputModel.hs & 102 \\
    & OutputModel\_test.hs & 138 \\
\hline
   Output Proof & OutputProof.hs & 147 \\
    & OutputProof\_test.hs & 55 \\
\hline
\end{tabular}
\end{center}
