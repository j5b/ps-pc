For this project, we considered the programming languages and technology that our program would use; and tools to use during the development process. To reduce risk and difficulties, we adopted features of agile software development and testing methods most suited to our project. As well as planned a schedule of how the project would progress by iterations, in order to meet the requirements of the final product.

\subsection{Technology Used}
\label{sec:techused}

We used several tools and languages in our program, with the main programming language being Haskell. Other than Haskell, the parser generator, Happy, was used to produce the Haskell code for the parser component of the program. For the output of models and proofs into more readable format, Graphviz and LaTeX were used respectively.

Haskell, was chosen as the main programming language primarily because Haskell's pattern matching naturally implements the handling of cases (such as proof rules and formulas) together with way we decided to structure the program into the main components. This way work on the different components can be implemented independently and simultaneously. Other reasons why Haskell was more appealing than other programming languages considered (Java, C, C++) were Haskell's compact code and each team member's relative levels of programming skills and experience in each language.

After choosing to use Haskell, GHC was picked as the compiler used in the development of the program. GHC was chosen over Hugs because it has a higher runtime performance, which was preferable since our program could be dealing with complex sets of concepts. Other reasons for our choice were GHC has a bigger and larger library and can compile to an executable independent of the library.

With no prior experience in programming a parser, we considered using Happy and Parsec in implementing the parser component for user input and benchmark files. Happy is an LR (bottom-up) parser generator and requires Happy to be run to generate the Haskell code for the parser, whilst Parsec is a LL (top-down) parser library for Haskell so does not require this pre-processing stage.

All members of our team have not used either of these before and so the decision to use Happy was based upon the available documentation. Happy's documentation was clearer and we observed that it involved simpler hand-written code. Hence, we believed the parser would be completed in a shorter amount of time by using Happy. The only disadvantage is that Happy needed to be run after any changes to the Happy file to generate, but this would only be required in the development stages and not when used by end users.

For the output of proofs into human readable format for end users, Latex was used so that the proofs could be easily integrated into other Latex documents. This is because our program was targeted at academia, we intended for the proofs to be a part of articles/documents written by the end user.

On the other hand, for the output of models, Graphviz was used. Graphviz is an open source graph visualisation software. Graphviz was chosen because graphs could be drawn by scripting them in simple DOT language to describe the graph, so was suitable for processing our model data structures and had a short learning time.

Other tools used to aid the software development method include Google Code to host our project, Mercurial version control, issue tracker to log complex bugs, \emph{hlint} and cabal. These are discussed in the Software Development and Testing Methods subsection.

\subsection{Technical Challenges}

We encountered a few technical challenges throughout the development of this project. The most problematic was the implementation of caching in the proof/model search component. This problem resulted in other technical difficulties, including in extending our program to allow users to express specific individuals that must exist and to reason with. We addressed these challenges by frequent informal meetings, in which we discussed the technical issues arisen and ways to resolve them such as through code reviews and bug tracking.

In the early iterations, we discovered the potential problem of cycles in the input global assumptions, which causes complications in the implementation of proof search, model construction and model checking. To prevent the proof/model search component from potentially becoming non-terminating, we decided to implement a mechanism for caching the sets of concepts already expanded and their resulting proof or model. This was the case where \textit{Gamma} contained an existential restriction concept, for example:

\textit{Givens} = \{\}

\textit{Gamma} = $\{ \exists knows . \top \}$

Extending the already written proof/model search component to cover this was very challenging because proof construction and model construction are performed simultaneously in our implementation. By implementing caching, the code became much more complex than before and as a result, it was difficult to reason the correctness of code and to debug for very specific cases.

This issue was tackled by discussion in our frequent group meetings and meetings with our supervisor; using code reviews and bug tracking system. Our discussions revolved around how global caching worked in the literature read (\cite{gore07, Gore:2010:OTA}), and how it would be applied to our program. In this way, all members took part and members of the group not assigned to the implementation of the proof/model search aided in locating and fixing bugs.

Another challenge we faced was when attempting to extend the program to express a new sort, individuals, in logic, enabling a specific individual to be expressed and reasoned with. In our meetings, we discussed the theoretical issues and how this would be implemented. The proof theory for individuals did not work with our design, and all data structures and components of the program needed to go through a great deal of changes. Given the limited time, it was decided that we will not implement this extension, but rather implement the other requirements set for the finished product.

\subsection{Software Development and Testing Methods}

To minimise risk and allow the project to adapt to changes quickly when difficulties occurred, a mixture of software development methods was adopted together with extensive testing methods. In addition, a variety of tools was used and code management policies were established to ensure quality and correctness of code.


The software development method adopted was a blend of mainly eXtreme Programming, Scrum and Crystal Clear, which were suitable for this project. Agile software development methods meant by using short (1 week) iterations, we ensured there was a working product at the end of each iteration.

From considering Unified Process method, we decided to implement the high risk elements in early iterations so that the most critical key requirements and most difficult aims are achieved first before implementing the features with a lower risk.

The test-driven nature of eXtreme Programming was a core part of our project development since the integrity of code is very important and directly affects whether the key requirements are met. Therefore, test suites for unit testing and test cases well known in the proof-search-writer community were used to test for correctness and performance. These unit tests were written before the coding of functions or simultaneously, and were tested as implemented. To allow an easy way of running the test suite, they were managed using cabal.

To complement with the unit tests, we also made use of \emph{hlint}, grey-box testing after each significant change and black-box testing to test our program with large complex inputs. We used \emph{hlint}, a program similar to lint, that analyses Haskell programs and suggests changes, to ensure a good and consistent style throughout the program. This not only ensured that the source code is easier to read and understand, but also made programming mistakes less likely.

We integrated these tools into a validation process. We ran \emph{hlint} and unit tests before submitting code to the repository. Then we gave code reviews and used grey-box testing to improve the quality of the code and find bugs that weren't found before. On a regular basis we ran the black-box tests on the whole program, discovering issues that were missed in the previous steps. This whole setup gave us high confidence that the program is working correctly.

Due to the possible difficulty of forming the algorithms in our project, we gave priority to having a simple solution with clear code first, a feature of eXtreme Programming, to guarantee the existence of a working version of the program.

Extensive code reviews were essential for improving quality of code and for all team members to understand each others code, under eXtreme Programming. It also led to the discovery of some small programming mistakes in the reviews. Hence, the project was hosted on Google Code throughout development, chosen for its features allowing easy submission for code reviews by other team members.

In addition, Google Code integrated well with our choice of version control, Mercurial and provided an issue tracker, which we used to report and track bugs. The benefits of using Mercurial were it allows local commits before pushing the changesets to the repository, with relatively easy-to-use graphical user interface extensions; and it is more flexible than other version controls, for example SVN.

From eXtreme Programming, pair programming was used for overlapping areas and more complex parts of the program; and also where sections of the code, implemented by different team members, meet. This ensured code written by different members would be as compatible as possible. Otherwise, each member was mainly responsible for a certain section of the program.

As in Scrum, we kept a product backlog, sprint backlog and also a `burn down list' to measure the actual progress and velocity of the overall project and iterations against. This gave each member clear specific tasks to complete that contributed towards meeting the requirements set for the project.

Regular scrum-like meetings were undertaken when we have a group meeting at least twice a week, focusing on:

\begin{itemize}
\item What has each member done since the last meeting (progress with respect to the assigned tasks)?
\item What will each member do until the next meeting?
\item Any technical and theoretical issues that have arisen and how to resolve them.
\end{itemize}

These short meetings encouraged progression of the project and highlighted technical problems early on. Care was taken to ensure workload was divided fairly taking into account different levels and areas of skills in the team, and team members were not underperforming. When issues found from testing such as integration bugs, we discussed and agreed on specific requirements of each component when differences in expectations arose.

In addition, we had weekly meetings with our supervisor in which we discussed next iterations and any issues regarding the code.

Here is the description of our scheduled meetings:
\begin{center}
\begin{tabular}{| c | l | r |}
\hline
Meeting 1 & Monday 12 pm & Group Members \\ \hline
Meeting 2 & Friday 4 pm & Group Members \\ \hline
Meeting 3 & Thursday 2 pm & Group Members and Supervisor \\ 
\hline
\end{tabular}
\end{center}

Extra meetings were scheduled as needed, in which we solved any problems that have risen from a gap in our technical or theoretical knowledge. This was to avoid the creation of delays in the project development and it is important for everyone to understand the code under eXtreme Programming.

When possible, we also adopted features of Crystal Clear, including being co-located and make reflective improvements. Even when pair programming was not use, by being co-located, immediate suggestions and help could be offered by other team members. Reflective improvements were made possible by reflecting upon what had been done since the last meeting and discussing what might work better. The agreed changes would then be implemented in the following iterations.

\subsection*{Testing Methods}

Since a test-driven development method was adopted, unit testing was done at all stages of development, using HUnit (a unit testing framework for Haskell). To complement unit testing, other testing methods included the use of grey-box and black-box testing. In this way, we were able to test at multiple levels: simple tests for base cases and more elaborate tests for a combination of simple inputs as unit tests; and a random selection of complex inputs from test generator and benchmark files containing formulas. As a result, about 3-10 small bugs were exposed in each component of the program by unit testing, with a total of approximately 30 bugs, whilst another 10 bugs were found by the other testing methods.

Unit tests were written before the implementation of functions or simultaneously; and test suites are run regularly, especially after any changes to code, to guarantee the program still works as expected. Unit tests were written for code coverage, ensuring all input cases were considered and our code produces the expected results. All areas of code were tested in this way individually, with a total of approximately 300 unit tests written.

Although writing extensive sets of unit tests that cover all possible cases took almost as much time as implementing the code itself, there were many benefits from this approach. By simultaneously writing the unit tests, it helped programmers think about and have a clear view of the expected/desired output of functions. This made coding of functions easier and members were able to identify mistakes quickly. Most of the time, bugs were found as the functions were implemented and were therefore corrected immediately.

When moving on to code other parts of the program, we have confidence that tested code is correct and produces expected results when given expected inputs. As a result, we were able to highlight differences in expectations between members writing different parts of the program for discussion in our meetings and come to an agreed solution.

In particular, having extensively tested proof checker and model checker by unit tests enabled these components of the program to be used with high confidence in other types of testing. The proof and model checkers are important in these other tests to check the results produced, for the automatically generated complex input cases, by the proof and model construction part of the program, which is very complicated and much harder to test simply by unit tests due to its exploratory nature.

Overall, unit testing revealed most bugs early on or immediately as code was being implemented or underwent a change, before code became more complex and difficult to resolve. These bugs included cases that were not covered properly, errors in the code, and bugs introduced due to changes in code.

Unit testing immediately revealed some forgotten cases in the initial code, such as:

\begin{itemize}
\item When falsity was in the set of inputs for the proof and model construction, a model was produced rather than the expected proof containing only falsity to show unsatisfiability.
\item The possibility of cycles in the input global assumptions was not dealt with by the proof and model construction stage.
\end{itemize}

Some mistakes in code that may have been overlooked without our approach to unit testing:

\begin{itemize}
\item After establishing that at each step of a proof, the set of concepts should not contain any duplicates, the proof searcher produced a result containing duplicated concepts as a result of applying the proof rule for existential restriction since a specific case was not considered. This was where the inputs contained:
\begin{itemize}
\item An existential restriction concept stating there exists a particular relation-successor that satisfies a concept, e.g. $\exists hasTopping . vegetable $
\item A universal restriction concept stating all successors of the same relation must satisfy the same concept specified by the exists concept, e.g. $ \forall hasTopping . vegetable $
\end{itemize}
\end{itemize}

After some changes to code, some bugs were introduced since the effect of the changes on all cases was not considered. For example:

\begin{itemize}
\item After combining the separate parsers for user input and benchmark files to reuse functions, the parsing for one of the benchmark files was incorrect and needed to have further changes.
\end{itemize}

Although the unit tests were very useful and helped us find many bugs an early stage, they don't help with more complex bugs. This is why we made use of grey/black box testing.

After each significant change we used grey-box testing to validate these changes, i.e. we tried, using our knowledge of the inner workings of the program, to find possible problems. This sometimes led to the discovery of small issues, for example a forgotten case and integration bugs. To exemplify, the proof/model search did not ensure there were no duplicated concepts produced after applying rules in the proof, whilst the proof checker detects this and indicated the proof was incorrect.

Furthermore, we used two different ways of black-box testing. We wrote a small program that automatically generates large test cases, runs the proof/model searcher on them, and then either the proof or the model checker on the result. We also wrote a parser, so we could read complex concepts from benchmark files\footnote{TANCS 2000 Problems: http://www.cs.man.ac.uk/~schmidt/mspass/problems.html}\footnote{logics work bench: http://iamwww.unibe.ch/~lwb/benchmarks/benchmarks.html} that are available from the proof searcher community, and test these in the same way.

Both these approaches to black-box testing turned out to be very useful and helped us discover some rather obscure bugs (e.g. bugs that would only occur if concepts were nested in a very particular and complex way). These kinds of tests can also be understood as stress testing of our program, because the cases are larger than we expect normal user input to be and are very likely to test concepts that go beyond what users would normally input. Once inputs were reduced to the smallest set that results in the error, they were added to the test suites as unit tests.

\subsection{Comparisons of Plans and Achievements}

A schedule of the iteration plans was composed in the early weeks at the beginning of the project, which would produce a finished product, meeting the set requirements. Using agile software development allowed us to adapt to changes in the initial plan due to technical difficulties of implementing caching and extending the program to deal with individuals.

The overall progression was hindered by these technical difficulties, causing planned iterations and achievements to be delayed by one iteration and the eventual decision to no longer implement the individuals extension. With these exceptions, we were able to complete the set tasks in our meetings, for each iteration.

Although, code reviews and testing were not specifically set in each iteration, it was established to be carried out throughout all iterations by our software development method. Where issues were found, such as integration bugs, they were discussed and members agree on a solution in following meetings. The implementations of solutions were then added as tasks in the next iteration, as well as other possible improvements for reflective improvement.

\subsection*{Iteration 1 (start 22/10, end 12/11)}

The original plan was to have two complete iterations by the 05/11and 12/11 respectively. In practice, we completed a single iteration which combined the tasks for both iterations by 12/11.

We intended to produce a working program that, given $\mathcal{AL}$ concepts, produces either a model or a proof showing its unsatisfiability, and both proof and model can be checked for correctness by the program. Only in the second iteration, where we to extend the systems functionality to deal with the more expressive  $\mathcal{ALC}$ logic. However, we found that a vast amount of refactoring and redesign will be required, if we were to move from  $\mathcal{AL}$ to  $\mathcal{ALC}$ in the second iteration. Therefore, we decided to produce one iteration, which would ultimately achieve a combined result of the first and second iterations.

Overall, we were able to complete the iteration successfully and all team members acquired a theoretical understanding of the task by reading articles on description logic and proof/model construction, (\cite{baadernutt02, gore99, Gore:2010:OTA, gore07}). We achieved, as planned, a tested program that constructs and checks a model/proof for ALC concepts with some bugs. Additionally, from unit testing, we discovered the problem of cycles in the global assumptions, causing non-termination in the proof/model search component.

\subsection*{Iteration 2 (start 12/11, end 19/11)}

In the draft schedule, we had planned to extend the system's functionality to cover a more expressive logic in this iteration. However, from implementing the first iteration, we discovered there was the potential problem of cycles in the input global assumptions. We decided to deal with in this iteration as addressing this potential problem later, after other extensions have been added, had the risk of causing the solution to be more difficult to attain and test. 

Much time was needed for us to read and discuss how the problem could be solved in the proof and model construction simultaneously, and work on adapting the proof/model search algorithm was started. This proved to be more complicated that we expected, consequently, this task was incomplete at the end of the iteration.

We were able to complete the other tasks to:
\begin{itemize}
\item Make the changes agreed to solve integration bugs and code improvement from the previous iteration
\item Set up cabal for package management and running all the tests together, which we found to have very inconsistent test suite setup and messages returned.
\end{itemize}

\subsection*{Iteration 3 (start 19/11, end 26/11)}

In this iteration, we had planned to extend the program to be able to express a new sort, individuals, in logic. This was to enable users to express and reason with a specific individual in the domain. However, since the previous iteration did not run smoothly, the task of implementation of caching was extended to this iteration.

To enable black-box testing to be performed with large complex input cases, a program that automatically generates large test cases; and a parser to extract large complex concepts from benchmark files were implemented. Hence, the implementation of the parser for user input was also moved to this iteration to be developed at the same time, rather than implemented as planned in the last iteration.

We were able to complete our tasks apart from solving the problem with cycles, which contained bugs after an initial implementation of caching:
\begin{itemize}
\item Improvements from the previous iteration to use clearer and consistent test suite setup and messages returned for all components.
\item A working parser and test generator to test our program, which found additional bugs in the proof/model search
\end{itemize}

\subsection*{Iteration 4 (start 26/11, end 3/12)}

This iteration was intentionally planned to extend the program to cover counting in logic only if the previous iteration progressed well. As expected, there were problems due to the technical nature of this project, so we started to plan how to extend our program to express individuals in the domain in this iteration.

After much time spent on reading and discussing the theory on individuals from \cite{Schmidt:2007:UTD:1785162.1785195}, we found that our code needed to undergo a substantial amount of change to successfully. Given the limited time, we decided not to continue with this extension since we believed it would not be completed in time and will affect other more desirable features that were to be implemented.

Most of the time was invested in fixing bugs found in the previous iteration; completing the implementation of caching in the proof/model search; and testing in this iteration. As a result, we were able to fix bugs in and complete caching, however, black-box testing revealed some obscure bugs in the proof/model search.

\subsection*{Iteration 5 (start 3/12, end 10/12)}

For the last iteration, we successfully integrated the additional features as planned, including the output of proofs and models into human readable format, and implementing the search for the shortest proof/smallest model. In addition, we were able to complete and fix the remaining bugs in the proof/search component.

Although, possible optimisation of code was continuously performed through code reviews, in this iteration, a higher level of code optimisation was done as the development was coming to an end.

To complete the project, we prepared the program for open-source deployment was done in this iteration by completing the main function and user interface. We achieved a working program that parses user input into the internal data structures used to construct and check proofs/models, which are returned to users in a readable format. In addition, our program allows the user to use the program to return the shortest proof or smallest model.


\subsection{Code Lengths}

During the development of the program, we found that most of the test suites were of a larger length of lines than their associated components, due to our extensive testing methods. Upon completion of all the iterations, our program consisted of the following components and lengths of code, which we believe to be relatively short as a result of using Haskell as our programming language:

\begin{center}
  \begin{tabular}{|l|c|}
\hline
\textbf{Component} & \textbf{Code Length (lines)} \\
\hline
Common code (utilities and data structures) & 337\\
\hline
Main & 334\\
\hline
Parser & 1111\\
\hline
Proof/Model Search & 234\\
\hline
Model Checker & 136\\
\hline
Proof Checker & 144\\
\hline
Output Model & 102\\
\hline
Output Proof & 147\\
\hline
Tests & 2338\\
\hline
Setup & 11\\
\hline
\end{tabular}
\end{center}

`Tests' is much larger than all other components since it includes the test generator and all the test suites containing a large set of unit tests. The parser component is also notably larger than the other components as it contains generated code from running Happy on the Happy file which was written by hand.
